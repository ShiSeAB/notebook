<!doctype html><html lang=zh class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Welcome to Shise's notebook. This site serves as a personal knowledge base for me to record my thoughts and ideas. It is also a place for me to share my knowledge and experience with the world. I hope you find something useful here. "><meta name=author content=ShiSeAB><link href=https://shiseab.github.io/notebook/DeepLearning/Attention%20Mechanisms/ rel=canonical><link rel=icon href=../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.14"><title>Attention Mechanisms - ShiSe的notebook</title><link rel=stylesheet href=../../assets/stylesheets/main.342714a4.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=LXGW+WenKai+Screen:300,300i,400,400i,700,700i%7CJetBrains+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"LXGW WenKai Screen";--md-code-font:"JetBrains Mono"}</style><link rel=stylesheet href=../../css/timeline.css><link rel=stylesheet href=https://gcore.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css><link rel=stylesheet href=https://gcore.jsdelivr.net/npm/lxgw-wenkai-screen-webfont@1.1.0/style.css><link rel=stylesheet href=https://gcore.jsdelivr.net/npm/lxgw-wenkai-webfont@1.1.0/style.css><link rel=stylesheet href=../../css/custom.css><link rel=stylesheet href=../../css/card.css><link rel=stylesheet href=../../css/tasklist.css><link rel=stylesheet href=../../css/flink.css><link rel=stylesheet href=../../css/more_changelog.css><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=default data-md-color-accent=default> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#attention-mechanisms class=md-skip> 跳转至 </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=页眉> <a href=../.. title=ShiSe的notebook class="md-header__button md-logo" aria-label=ShiSe的notebook data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> ShiSe的notebook </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Attention Mechanisms </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=default data-md-color-accent=default aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=搜索 placeholder=搜索 autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=查找> <button type=reset class="md-search__icon md-icon" title=清空当前内容 aria-label=清空当前内容 tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> 正在初始化搜索引擎 </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/ShiSeAB/notebook.git/ title=前往仓库 class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> ShiSeAB/notebook </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=标签 data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../.. class=md-tabs__link> HOME </a> </li> <li class=md-tabs__item> <a href=../../RL/basic_1/ class=md-tabs__link> AI </a> </li> <li class=md-tabs__item> <a href=../../Compiler/Introduction/ class=md-tabs__link> CS课程 </a> </li> <li class=md-tabs__item> <a href=../../essays/DeepSeek-R1/ class=md-tabs__link> 论文阅读 </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=导航栏 data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title=ShiSe的notebook class="md-nav__button md-logo" aria-label=ShiSe的notebook data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> ShiSe的notebook </label> <div class=md-nav__source> <a href=https://github.com/ShiSeAB/notebook.git/ title=前往仓库 class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> ShiSeAB/notebook </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1> <div class="md-nav__link md-nav__container"> <a href=../.. class="md-nav__link "> <span class=md-ellipsis> HOME </span> </a> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=false> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> HOME </label> <ul class=md-nav__list data-md-scrollfix> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> <span class=md-ellipsis> AI </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> AI </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_1> <label class=md-nav__link for=__nav_2_1 id=__nav_2_1_label tabindex=0> <span class=md-ellipsis> 强化学习 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_1_label aria-expanded=false> <label class=md-nav__title for=__nav_2_1> <span class="md-nav__icon md-icon"></span> 强化学习 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../RL/basic_1/ class=md-nav__link> <span class=md-ellipsis> 基础知识 </span> </a> </li> <li class=md-nav__item> <a href=../../RL/TRPO/ class=md-nav__link> <span class=md-ellipsis> TRPO </span> </a> </li> <li class=md-nav__item> <a href=../../RL/PPO/ class=md-nav__link> <span class=md-ellipsis> PPO </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2> <label class=md-nav__link for=__nav_2_2 id=__nav_2_2_label tabindex=0> <span class=md-ellipsis> 深度学习 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2> <span class="md-nav__icon md-icon"></span> 深度学习 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../Convolution/ class=md-nav__link> <span class=md-ellipsis> 卷积神经网络 </span> </a> </li> <li class=md-nav__item> <a href=../Modern%20CNN/ class=md-nav__link> <span class=md-ellipsis> 现代卷积神经网络架构 </span> </a> </li> <li class=md-nav__item> <a href=../Recurrent%20neural%20network/ class=md-nav__link> <span class=md-ellipsis> 循环神经网络 </span> </a> </li> <li class=md-nav__item> <a href=../Modern%20RNN/ class=md-nav__link> <span class=md-ellipsis> 现代循环神经网络 </span> </a> </li> <li class=md-nav__item> <a href="../Attention Mechanisms" class=md-nav__link> <span class=md-ellipsis> 注意力机制 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> <span class=md-ellipsis> CS课程 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> CS课程 </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_1> <label class=md-nav__link for=__nav_3_1 id=__nav_3_1_label tabindex=0> <span class=md-ellipsis> 编译原理 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_1_label aria-expanded=false> <label class=md-nav__title for=__nav_3_1> <span class="md-nav__icon md-icon"></span> 编译原理 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Compiler/Introduction/ class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class=md-nav__item> <a href=../../Compiler/Lexical_Analysis/ class=md-nav__link> <span class=md-ellipsis> 词法分析 </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_1_3> <label class=md-nav__link for=__nav_3_1_3 id=__nav_3_1_3_label tabindex=0> <span class=md-ellipsis> 语法分析 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_3_1_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3_1_3> <span class="md-nav__icon md-icon"></span> 语法分析 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Compiler/Parsing/ class=md-nav__link> <span class=md-ellipsis> Top-Down </span> </a> </li> <li class=md-nav__item> <a href=../../Compiler/Parsing-2/ class=md-nav__link> <span class=md-ellipsis> Bottom-Up </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../Compiler/Abstract_Syntax/ class=md-nav__link> <span class=md-ellipsis> 抽象语法 </span> </a> </li> <li class=md-nav__item> <a href=../../Compiler/Semantic_Analysis/ class=md-nav__link> <span class=md-ellipsis> 语义分析 </span> </a> </li> <li class=md-nav__item> <a href=../../Compiler/Activation_Record/ class=md-nav__link> <span class=md-ellipsis> 活动记录 </span> </a> </li> <li class=md-nav__item> <a href=../../Compiler/ch7-IR/ class=md-nav__link> <span class=md-ellipsis> 中间代码生成 </span> </a> </li> <li class=md-nav__item> <a href=../../Compiler/ch8/ class=md-nav__link> <span class=md-ellipsis> 基本块 </span> </a> </li> <li class=md-nav__item> <a href=../../Compiler/ch9/ class=md-nav__link> <span class=md-ellipsis> 指令选择 </span> </a> </li> <li class=md-nav__item> <a href=../../Compiler/ch10/ class=md-nav__link> <span class=md-ellipsis> 活跃变量分析 </span> </a> </li> <li class=md-nav__item> <a href=../../Compiler/ch11/ class=md-nav__link> <span class=md-ellipsis> 寄存器分配 </span> </a> </li> <li class=md-nav__item> <a href=../../Compiler/ch13/ class=md-nav__link> <span class=md-ellipsis> 垃圾回收 </span> </a> </li> <li class=md-nav__item> <a href=../../Compiler/ch14/ class=md-nav__link> <span class=md-ellipsis> 面向对象语言 </span> </a> </li> <li class=md-nav__item> <a href=../../Compiler/ch18/ class=md-nav__link> <span class=md-ellipsis> 循环优化 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_2> <label class=md-nav__link for=__nav_3_2 id=__nav_3_2_label tabindex=0> <span class=md-ellipsis> 自然语言处理导论 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_2_label aria-expanded=false> <label class=md-nav__title for=__nav_3_2> <span class="md-nav__icon md-icon"></span> 自然语言处理导论 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../nlp/Deep%20Learning%20Basic/ class=md-nav__link> <span class=md-ellipsis> 深度学习基础 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex=0> <span class=md-ellipsis> 论文阅读 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> 论文阅读 </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_1> <label class=md-nav__link for=__nav_4_1 id=__nav_4_1_label tabindex=0> <span class=md-ellipsis> RL </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_1_label aria-expanded=false> <label class=md-nav__title for=__nav_4_1> <span class="md-nav__icon md-icon"></span> RL </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../essays/DeepSeek-R1/ class=md-nav__link> <span class=md-ellipsis> DeepSeek-R1 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2> <label class=md-nav__link for=__nav_4_2 id=__nav_4_2_label tabindex=0> <span class=md-ellipsis> CoT </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_2_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2> <span class="md-nav__icon md-icon"></span> CoT </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../essays/TokenSkip/ class=md-nav__link> <span class=md-ellipsis> TokenSkip </span> </a> </li> <li class=md-nav__item> <a href=../../essays/DEER/ class=md-nav__link> <span class=md-ellipsis> DEER </span> </a> </li> <li class=md-nav__item> <a href=../../essays/ThoughtTerminator/ class=md-nav__link> <span class=md-ellipsis> ThoughtTerminator </span> </a> </li> <li class=md-nav__item> <a href=../../essays/SEAL/ class=md-nav__link> <span class=md-ellipsis> SEAL </span> </a> </li> <li class=md-nav__item> <a href=../../essays/MRL/ class=md-nav__link> <span class=md-ellipsis> MRT </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_3> <label class=md-nav__link for=__nav_4_3 id=__nav_4_3_label tabindex=0> <span class=md-ellipsis> Social </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_3_label aria-expanded=false> <label class=md-nav__title for=__nav_4_3> <span class="md-nav__icon md-icon"></span> Social </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../essays/Social/SocialGenome/ class=md-nav__link> <span class=md-ellipsis> SocialGenome </span> </a> </li> <li class=md-nav__item> <a href=../../essays/Social/MiMeQA/ class=md-nav__link> <span class=md-ellipsis> MiMeQA </span> </a> </li> <li class=md-nav__item> <a href=../../essays/Social/EgoToM/ class=md-nav__link> <span class=md-ellipsis> EgoToM </span> </a> </li> <li class=md-nav__item> <a href=../../essays/Social/Text-Social%20Benchmark/ class=md-nav__link> <span class=md-ellipsis> TextSocial </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label=目录> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> 目录 </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#1 class=md-nav__link> <span class=md-ellipsis> 1. 注意力汇聚 </span> </a> <nav class=md-nav aria-label="1. 注意力汇聚"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#31-nonparametric-attention-pooling class=md-nav__link> <span class=md-ellipsis> 3.1 Nonparametric attention pooling </span> </a> </li> <li class=md-nav__item> <a href=#12-parametric-attention-pooling class=md-nav__link> <span class=md-ellipsis> 1.2 Parametric attention pooling </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#2 class=md-nav__link> <span class=md-ellipsis> 2. 注意力分数 </span> </a> <nav class=md-nav aria-label="2. 注意力分数"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#21-additive-attention class=md-nav__link> <span class=md-ellipsis> 2.1 Additive Attention </span> </a> </li> <li class=md-nav__item> <a href=#22-scaled-dot-product-attention class=md-nav__link> <span class=md-ellipsis> 2.2 Scaled Dot-Product Attention </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#3-bahdanau class=md-nav__link> <span class=md-ellipsis> 3. Bahdanau 注意力 </span> </a> <nav class=md-nav aria-label="3. Bahdanau 注意力"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#_1 class=md-nav__link> <span class=md-ellipsis> 代码 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#4-self-attention-and-position-encoding class=md-nav__link> <span class=md-ellipsis> 4. Self-Attention and Position Encoding </span> </a> <nav class=md-nav aria-label="4. Self-Attention and Position Encoding"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#41-self-attention class=md-nav__link> <span class=md-ellipsis> 4.1 Self-Attention </span> </a> <nav class=md-nav aria-label="4.1 Self-Attention"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#cnnrnn class=md-nav__link> <span class=md-ellipsis> 与CNN，RNN对比 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=attention-mechanisms>Attention Mechanisms<a class=headerlink href=#attention-mechanisms title="Permanent link">&para;</a></h1> <p>在心理学上，动物需要在复杂环境下有效关注值得注意的点。</p> <ul> <li>人类根据 <strong>非自主性提示</strong> 和 <strong>自主性提示</strong> 选择注意点</li> <li><strong>非自主性提示</strong> 是基于环境中物体的突出性和易见性，引起人的注意</li> <li><strong>自主性提示</strong> 则是受主观意愿推动去注意</li> </ul> <p>卷积、全连接、池化层都只考虑“非自主性提示”，哪个显眼就注意哪个。</p> <p>而注意力机制则考虑 “自主性提示”：</p> <ul> <li>该提示被称为 <strong>查询query</strong> </li> <li>每个输入是一个 <strong>值value</strong> 和非自主性提示 <strong>key</strong> 的对，即key-value pair</li> <li>通过注意力池化层来有偏向性的选择某些输入</li> </ul> <p><img alt=image-20250319125256346 src=../Attention%20Mechanisms.assets/image-20250319125256346.png></p> <h2 id=1>1. 注意力汇聚<a class=headerlink href=#1 title="Permanent link">&para;</a></h2> <p>Average pooling 肯定是不行的，没有将key和value联系起来。</p> <h3 id=31-nonparametric-attention-pooling>3.1 Nonparametric attention pooling<a class=headerlink href=#31-nonparametric-attention-pooling title="Permanent link">&para;</a></h3> <p>用Nadaraya-Watson核回归实现非参数的注意力汇聚：</p> <p><img alt=image-20250319125501985 src=../Attention%20Mechanisms.assets/image-20250319125501985.png></p> <ul> <li>给定数据 <span class=arithmatex>\((x_i,y_i)\)</span> ，根据输入的位置对输出 <span class=arithmatex>\(y_i\)</span> 进行加权。 <span class=arithmatex>\(K\)</span> 是一个衡量距离的核函数。</li> </ul> <p>受此启发，得到通用 attention pooling 公式： $$ f(x) = \sum_{i=1}^{n}\alpha(x,x_i)y_i $$</p> <ul> <li>attention pooling 是 <span class=arithmatex>\(y_i\)</span> 的加权平均</li> <li>将查询 <span class=arithmatex>\(x\)</span> 与键 <span class=arithmatex>\(x_i\)</span> 之间的关系建模为 attention weight <span class=arithmatex>\(\alpha(x,x_i)\)</span> ，这个权重将被分配给每一个对应值 <span class=arithmatex>\(y_i\)</span></li> <li>对于任何查询，模型在所有键值对注意力权重都是一个有效的概率分布： 它们是非负的，并且总和为1</li> </ul> <p>如果使用高斯核：</p> <p><img alt=image-20250319165925497 src=../Attention%20Mechanisms.assets/image-20250319165925497.png></p> <p>一个键 <span class=arithmatex>\(x_i\)</span> 越是接近给定的查询 <span class=arithmatex>\(x\)</span> ， 那么分配给这个键对应值 <span class=arithmatex>\(y_i\)</span> 的注意力权重就会越大， 也就“获得了更多的注意力”。</p> <h3 id=12-parametric-attention-pooling>1.2 Parametric attention pooling<a class=headerlink href=#12-parametric-attention-pooling title="Permanent link">&para;</a></h3> <p>将可学习的参数集成到注意力汇聚中，在查询 <span class=arithmatex>\(x\)</span> 和键 <span class=arithmatex>\(x_i\)</span> 之间的距离乘以可学习参数 <span class=arithmatex>\(w\)</span> ，再通过训练模型来学习参数。</p> <p>与非参数的注意力汇聚模型相比， 带参数的模型加入可学习的参数后， 曲线在注意力权重较大的区域变得更不平滑。</p> <h2 id=2>2. 注意力分数<a class=headerlink href=#2 title="Permanent link">&para;</a></h2> <p><img alt=image-20250319192030164 src=../Attention%20Mechanisms.assets/image-20250319192030164.png></p> <p>通过 Attention scoring function 实现对查询和键之间的关系建模(评估query和key的相似度)，将其结果输入到 softmax 中得到注意力权重. 最后，注意力汇聚的输出就是基于这些注意力权重的值的加权和。</p> <p><img alt=image-20250319193237838 src=../Attention%20Mechanisms.assets/image-20250319193237838.png></p> <p><span class=arithmatex>\(\alpha(q,k_i)\)</span> : 将 <span class=arithmatex>\(q\)</span> 和 <span class=arithmatex>\(k_i\)</span> 两个向量输入注意力评分函数 <span class=arithmatex>\(a\)</span> , 将其映射成标量, 再经softmax运算得到注意力权重.</p> <h3 id=21-additive-attention>2.1 Additive Attention<a class=headerlink href=#21-additive-attention title="Permanent link">&para;</a></h3> <p>当查询和键是 <strong>不同长度的矢量</strong> 时，可以使用加性注意力作为评分函数。 给定query <span class=arithmatex>\(\bf{q}\in \mathbb{R}^q\)</span>, key <span class=arithmatex>\(\bf{k} \in \mathbb{R}^k\)</span> :</p> <p><img alt=image-20250319193926714 src=../Attention%20Mechanisms.assets/image-20250319193926714.png></p> <p>等价于将key和query合并起来后放到一个 <strong>隐藏大小为 h</strong>, <strong>输出大小为1</strong> 的单隐藏层MLP.</p> <h3 id=22-scaled-dot-product-attention>2.2 Scaled Dot-Product Attention<a class=headerlink href=#22-scaled-dot-product-attention title="Permanent link">&para;</a></h3> <p>当查询和键是 <strong>相同长度</strong> 为d时, 可以使用缩放点积注意力. </p> <p><img alt=image-20250319195021596 src=../Attention%20Mechanisms.assets/image-20250319195021596.png></p> <p>假设查询和键的所有元素都是独立的随机变量， 并且都满足零均值和单位方差， 那么两个向量的点积的均值为0，方差为d。 为确保无论向量长度如何， 点积的方差在不考虑向量长度的情况下仍然是1.</p> <p><img alt=image-20250319195121419 src=../Attention%20Mechanisms.assets/image-20250319195121419.png></p> <h2 id=3-bahdanau>3. Bahdanau 注意力<a class=headerlink href=#3-bahdanau title="Permanent link">&para;</a></h2> <p><img alt=image-20250320111510510 src=../Attention%20Mechanisms.assets/image-20250320111510510.png></p> <p>在原始模型中, 上下文变量 <span class=arithmatex>\(\bf{c}\)</span> 用于表示 source sentence 的信息; 而在该架构中:</p> <ul> <li><span class=arithmatex>\(\bf{h_t}\)</span> 表示编码器在时间步 t 的隐藏层信息, 且既表示key又表示value;</li> <li><span class=arithmatex>\(s_{t'-1}\)</span> 表示解码器在时间步 t'-1 的隐藏层信息, 用于表示 query;</li> <li>在每个 decoding time step <span class=arithmatex>\(t'\)</span>, <span class=arithmatex>\(\bf{c}_{t'}\)</span> 都会被更新. 假设input sequence 长度为 <span class=arithmatex>\(T\)</span>:</li> </ul> <div class=arithmatex>\[ \bf{c}_{t'} = \sum_{t=1}^{T}\alpha(s_{t'-1},h_t)h_t \]</div> <ul> <li><span class=arithmatex>\(\bf{c}_{t'}\)</span> 将被输入解码器的循环层, 来生成解码器时间步 t' 的 state <span class=arithmatex>\(s_{t'}\)</span></li> </ul> <h4 id=_1>代码<a class=headerlink href=#_1 title="Permanent link">&para;</a></h4> <div class=highlight><pre><span></span><code><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=k>class</span><span class=w> </span><span class=nc>Seq2SeqAttentionDecoder</span><span class=p>(</span><span class=n>AttentionDecoder</span><span class=p>):</span>
<a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a>    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>,</span> <span class=n>embed_size</span><span class=p>,</span> <span class=n>num_hiddens</span><span class=p>,</span> <span class=n>num_layers</span><span class=p>,</span>
<a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a>                 <span class=n>dropout</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>):</span>
<a id=__codelineno-0-4 name=__codelineno-0-4 href=#__codelineno-0-4></a>        <span class=nb>super</span><span class=p>(</span><span class=n>Seq2SeqAttentionDecoder</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=o>**</span><span class=n>kwargs</span><span class=p>)</span>
<a id=__codelineno-0-5 name=__codelineno-0-5 href=#__codelineno-0-5></a>        <span class=c1>#采用additive attention为评分函数</span>
<a id=__codelineno-0-6 name=__codelineno-0-6 href=#__codelineno-0-6></a>        <span class=bp>self</span><span class=o>.</span><span class=n>attention</span> <span class=o>=</span> <span class=n>d2l</span><span class=o>.</span><span class=n>AdditiveAttention</span><span class=p>(</span>
<a id=__codelineno-0-7 name=__codelineno-0-7 href=#__codelineno-0-7></a>            <span class=n>num_hiddens</span><span class=p>,</span> <span class=n>num_hiddens</span><span class=p>,</span> <span class=n>num_hiddens</span><span class=p>,</span> <span class=n>dropout</span><span class=p>)</span>
<a id=__codelineno-0-8 name=__codelineno-0-8 href=#__codelineno-0-8></a>        <span class=bp>self</span><span class=o>.</span><span class=n>embedding</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>embed_size</span><span class=p>)</span>
<a id=__codelineno-0-9 name=__codelineno-0-9 href=#__codelineno-0-9></a>        <span class=bp>self</span><span class=o>.</span><span class=n>rnn</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>GRU</span><span class=p>(</span>
<a id=__codelineno-0-10 name=__codelineno-0-10 href=#__codelineno-0-10></a>            <span class=n>embed_size</span> <span class=o>+</span> <span class=n>num_hiddens</span><span class=p>,</span> <span class=n>num_hiddens</span><span class=p>,</span> <span class=n>num_layers</span><span class=p>,</span>
<a id=__codelineno-0-11 name=__codelineno-0-11 href=#__codelineno-0-11></a>            <span class=n>dropout</span><span class=o>=</span><span class=n>dropout</span><span class=p>)</span>
<a id=__codelineno-0-12 name=__codelineno-0-12 href=#__codelineno-0-12></a>        <span class=bp>self</span><span class=o>.</span><span class=n>dense</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>num_hiddens</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>)</span>
<a id=__codelineno-0-13 name=__codelineno-0-13 href=#__codelineno-0-13></a>
<a id=__codelineno-0-14 name=__codelineno-0-14 href=#__codelineno-0-14></a>    <span class=k>def</span><span class=w> </span><span class=nf>init_state</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>enc_outputs</span><span class=p>,</span> <span class=n>enc_valid_lens</span><span class=p>,</span> <span class=o>*</span><span class=n>args</span><span class=p>):</span>
<a id=__codelineno-0-15 name=__codelineno-0-15 href=#__codelineno-0-15></a>        <span class=c1># outputs的形状为(batch_size，num_steps，num_hiddens).</span>
<a id=__codelineno-0-16 name=__codelineno-0-16 href=#__codelineno-0-16></a>        <span class=c1># hidden_state的形状为(num_layers，batch_size，num_hiddens)</span>
<a id=__codelineno-0-17 name=__codelineno-0-17 href=#__codelineno-0-17></a>        <span class=n>outputs</span><span class=p>,</span> <span class=n>hidden_state</span> <span class=o>=</span> <span class=n>enc_outputs</span>
<a id=__codelineno-0-18 name=__codelineno-0-18 href=#__codelineno-0-18></a>        <span class=k>return</span> <span class=p>(</span><span class=n>outputs</span><span class=o>.</span><span class=n>permute</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>2</span><span class=p>),</span> <span class=n>hidden_state</span><span class=p>,</span> <span class=n>enc_valid_lens</span><span class=p>)</span>
<a id=__codelineno-0-19 name=__codelineno-0-19 href=#__codelineno-0-19></a>
<a id=__codelineno-0-20 name=__codelineno-0-20 href=#__codelineno-0-20></a>    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>state</span><span class=p>):</span>
<a id=__codelineno-0-21 name=__codelineno-0-21 href=#__codelineno-0-21></a>        <span class=c1># enc_outputs的形状为(batch_size,num_steps,num_hiddens).</span>
<a id=__codelineno-0-22 name=__codelineno-0-22 href=#__codelineno-0-22></a>        <span class=c1># hidden_state的形状为(num_layers,batch_size,</span>
<a id=__codelineno-0-23 name=__codelineno-0-23 href=#__codelineno-0-23></a>        <span class=c1># num_hiddens)</span>
<a id=__codelineno-0-24 name=__codelineno-0-24 href=#__codelineno-0-24></a>        <span class=n>enc_outputs</span><span class=p>,</span> <span class=n>hidden_state</span><span class=p>,</span> <span class=n>enc_valid_lens</span> <span class=o>=</span> <span class=n>state</span>
<a id=__codelineno-0-25 name=__codelineno-0-25 href=#__codelineno-0-25></a>        <span class=c1># 输出X的形状为(num_steps,batch_size,embed_size)</span>
<a id=__codelineno-0-26 name=__codelineno-0-26 href=#__codelineno-0-26></a>        <span class=n>X</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>embedding</span><span class=p>(</span><span class=n>X</span><span class=p>)</span><span class=o>.</span><span class=n>permute</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
<a id=__codelineno-0-27 name=__codelineno-0-27 href=#__codelineno-0-27></a>        <span class=n>outputs</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>_attention_weights</span> <span class=o>=</span> <span class=p>[],</span> <span class=p>[]</span>
<a id=__codelineno-0-28 name=__codelineno-0-28 href=#__codelineno-0-28></a>        <span class=k>for</span> <span class=n>x</span> <span class=ow>in</span> <span class=n>X</span><span class=p>:</span>
<a id=__codelineno-0-29 name=__codelineno-0-29 href=#__codelineno-0-29></a>            <span class=c1># query的形状为(batch_size,1,num_hiddens)</span>
<a id=__codelineno-0-30 name=__codelineno-0-30 href=#__codelineno-0-30></a>            <span class=n>query</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=n>hidden_state</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
<a id=__codelineno-0-31 name=__codelineno-0-31 href=#__codelineno-0-31></a>            <span class=c1># context的形状为(batch_size,1,num_hiddens)</span>
<a id=__codelineno-0-32 name=__codelineno-0-32 href=#__codelineno-0-32></a>            <span class=c1># enc_valid_lens是大小为batch_size的向量,每个值表示该样本句子有效长度,算有效长度内的注意力权重,大于长度的部分可以不要管</span>
<a id=__codelineno-0-33 name=__codelineno-0-33 href=#__codelineno-0-33></a>            <span class=n>context</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>attention</span><span class=p>(</span>
<a id=__codelineno-0-34 name=__codelineno-0-34 href=#__codelineno-0-34></a>                <span class=n>query</span><span class=p>,</span> <span class=n>enc_outputs</span><span class=p>,</span> <span class=n>enc_outputs</span><span class=p>,</span> <span class=n>enc_valid_lens</span><span class=p>)</span>
<a id=__codelineno-0-35 name=__codelineno-0-35 href=#__codelineno-0-35></a>            <span class=c1># 在特征维度上连结</span>
<a id=__codelineno-0-36 name=__codelineno-0-36 href=#__codelineno-0-36></a>            <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>context</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)),</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
<a id=__codelineno-0-37 name=__codelineno-0-37 href=#__codelineno-0-37></a>            <span class=c1># 将x变形为(1,batch_size,embed_size+num_hiddens)</span>
<a id=__codelineno-0-38 name=__codelineno-0-38 href=#__codelineno-0-38></a>            <span class=n>out</span><span class=p>,</span> <span class=n>hidden_state</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>rnn</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>permute</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>2</span><span class=p>),</span> <span class=n>hidden_state</span><span class=p>)</span>
<a id=__codelineno-0-39 name=__codelineno-0-39 href=#__codelineno-0-39></a>            <span class=n>outputs</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
<a id=__codelineno-0-40 name=__codelineno-0-40 href=#__codelineno-0-40></a>            <span class=bp>self</span><span class=o>.</span><span class=n>_attention_weights</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>attention</span><span class=o>.</span><span class=n>attention_weights</span><span class=p>)</span>
<a id=__codelineno-0-41 name=__codelineno-0-41 href=#__codelineno-0-41></a>        <span class=c1># 全连接层变换后，outputs的形状为</span>
<a id=__codelineno-0-42 name=__codelineno-0-42 href=#__codelineno-0-42></a>        <span class=c1># (num_steps,batch_size,vocab_size)</span>
<a id=__codelineno-0-43 name=__codelineno-0-43 href=#__codelineno-0-43></a>        <span class=n>outputs</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dense</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>0</span><span class=p>))</span>
<a id=__codelineno-0-44 name=__codelineno-0-44 href=#__codelineno-0-44></a>        <span class=k>return</span> <span class=n>outputs</span><span class=o>.</span><span class=n>permute</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>2</span><span class=p>),</span> <span class=p>[</span><span class=n>enc_outputs</span><span class=p>,</span> <span class=n>hidden_state</span><span class=p>,</span>
<a id=__codelineno-0-45 name=__codelineno-0-45 href=#__codelineno-0-45></a>                                          <span class=n>enc_valid_lens</span><span class=p>]</span>
<a id=__codelineno-0-46 name=__codelineno-0-46 href=#__codelineno-0-46></a>
<a id=__codelineno-0-47 name=__codelineno-0-47 href=#__codelineno-0-47></a>    <span class=nd>@property</span>
<a id=__codelineno-0-48 name=__codelineno-0-48 href=#__codelineno-0-48></a>    <span class=k>def</span><span class=w> </span><span class=nf>attention_weights</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
<a id=__codelineno-0-49 name=__codelineno-0-49 href=#__codelineno-0-49></a>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>_attention_weights</span>
</code></pre></div> <h2 id=4-self-attention-and-position-encoding>4. Self-Attention and Position Encoding<a class=headerlink href=#4-self-attention-and-position-encoding title="Permanent link">&para;</a></h2> <h3 id=41-self-attention>4.1 Self-Attention<a class=headerlink href=#41-self-attention title="Permanent link">&para;</a></h3> <p><img alt=image-20250320232546690 src=../Attention%20Mechanisms.assets/image-20250320232546690.png></p> <p>给定输入 <em>tokens序列</em> <span class=arithmatex>\(x_1,...,x_n \in \mathbb{R}^d\)</span> （为什么token是长为d的向量，因为words通过embedding变成词向量），该序列的自注意力输出为一个长度相同的序列 <span class=arithmatex>\(y_1,...,y_n\)</span> ， 其中：</p> <div class=arithmatex>\[ y_i = f(x_i,(x_1,x_1),...,(x_n,x_n)) \in \mathbb{R}^d \]</div> <p>将 <span class=arithmatex>\(x_i\)</span> 当作key，value，query来对序列抽取特征来得到输出序列。</p> <h4 id=cnnrnn>与CNN，RNN对比<a class=headerlink href=#cnnrnn title="Permanent link">&para;</a></h4> <p><img alt=image-20250320233635483 src=../Attention%20Mechanisms.assets/image-20250320233635483.png></p> <p><img alt=image-20250320233422941 src=../Attention%20Mechanisms.assets/image-20250320233422941.png></p> <ul> <li>GPU可以作并行运算，所以有并行度来衡量</li> <li>最长路径指，<span class=arithmatex>\(x_1\)</span> 到 <span class=arithmatex>\(x_n\)</span> 需要经过多少</li> </ul> <!-- Giscus --> <h2 id=__comments>评论</h2> <!-- 这里改成你的Giscus代码 --> <script src=https://giscus.app/client.js data-repo=HobbitQia/notebook data-repo-id=R_kgDOHtZjDg data-category=General data-category-id=DIC_kwDOHtZjDs4CQZ7Z data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=top data-theme=light data-lang=zh-CN crossorigin=anonymous async>
</script> <!-- Reload on palette change --> <script>
    var palette = __md_get("__palette")
    if (palette && typeof palette.color === "object")
        if (palette.color.scheme === "slate") {
            var giscus = document.querySelector("script[src*=giscus]")
            giscus.setAttribute("data-theme", "dark")
        }

    /* Register event handlers after documented loaded */
    document.addEventListener("DOMContentLoaded", function () {
        var ref = document.querySelector("[data-md-component=palette]")
        ref.addEventListener("change", function () {
            var palette = __md_get("__palette")
            if (palette && typeof palette.color === "object") {
                var theme = palette.color.scheme === "slate" ? "dark" : "light"

                /* Instruct Giscus to change theme */
                var frame = document.querySelector(".giscus-frame")
                frame.contentWindow.postMessage(
                    { giscus: { setConfig: { theme } } },
                    "https://giscus.app"
                )
            }
        })
    })
</script> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> 回到页面顶部 </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2023 ~ now | 🚀 Chen Wu (ShiSe) </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../..", "features": ["content.code.annotate", "navigation.tracking", "navigation.tabs", "navigation.indexes", "navigation.top"], "search": "../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}, "version": null}</script> <script src=../../assets/javascripts/bundle.13a4f30d.min.js></script> <script src=../../js/baidu-tongji.js></script> <script src=../../js/katex.js></script> <script src=../../js/mathjax.js></script> <script src=https://gcore.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script> </body> </html>