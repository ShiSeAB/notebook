# 卷积神经网络

### 1. 多层感知机的限制

- 输入是一维向量 $x \in \mathbb{R}^{1*d}$的情况下，权重 $W\in \mathbb{R}^{h*d}$ (h为向量 $h$ 的大小)只需要为二维张量:

  ![image-20250301145818180](./Convolution.assets/image-20250301145818180.png)

> ​	$h_j = \sum_{j}{W_{i,j}{x_j}} + b_j$

- 在输入是二维图像 $X$ 的情况下，隐藏层表示 $H$ 在数学上是一个矩阵，在代码中表示为二维张量。使用 $[X]_{i,j}$ 和 $[H]_{i,j}$ 分别表示输入图像和隐藏表示中位置 $(i,j)$ 处的像素。此时要求权重 $W$ 为一个四维张量：

  ![image-20250301150053475](./Convolution.assets/image-20250301150053475.png)

  为了节省参数（避免参数随着输入变大而变得无限多），我们假设有一个”检测器“扫描图像，将图像分割成多个区域(blocks)，并为每个区域包含目标的可能性打分。而索引a和b通过在正偏移和负偏移之间移动覆盖了整个图像，对于隐藏表示中任意给定位置 $( i , j )$ 处的像素值 $[H]_{i,j}$ , 通过对 $X$ 中以 $(i,j)$ 为中心的像素进行加权求和得到，权重为 $v_{i,j,a,b}$ , 实现了 ”检测器“ 功能。

  其中，从 $W$ 到 $V$ 的转化至少形式上的转化，这两个四阶张量的元素之间存在一一对应的关系：
  
  
  
  $$
  [V]_{i,j,a,b} = [W]_{i,j,i+a,j+b}
  $$
  



#### 1.1 平移不变性 Translation Invariance

​	检测对象在输入 $X$ 中的平移，应该仅导致隐藏表示 $H$ 中的平移。也就是说不管出现在图像中的哪个位置，神经网络的底层应该对相同的图像区域做出类似的响应。

​	但如公式 $h_{i,j} = \sum_{a,b}{v_{i,j,a,b}{x_{i+a,j+b}}}$ 所示，$i，j$的变化会导致 权重 $v$ 发生变化，使得到的 $h$ 也发生变化，违反了平移不变性。

​	所以，$v$ 不应该依赖于 $(i,j)$，定义：$v_{i,j,a,b} = v_{a,b}$，得到：



$$
h_{i,j} = \sum_{a,b}{v_{a,b}}{x_{i+a,j+b}}
$$
​	

这就是2维交叉相关，增加了重复性，减少参数数量，降低了模型复杂度



#### 1.2 局部性

​	神经网络的底层应该只探索输入图像中的局部区域，而不考虑图像远处区域的内容。为了收集用来训练参数 $[H]_{i,j}$ 的相关信息，

在 $|a|> \Delta$ 和 $|b| > \Delta$ 的范围之外，我们可以设置 $[V]_{a,b} = 0$，故将公式重写为：



$$
[H]_{a,j} = u + \sum_{a = -\Delta}^{\Delta}\sum_{b = -\Delta}^{\Delta}[V]_{a,b}[X]_{i+a,j+b}
$$
​	

​	$V$ 被称为**卷积核**（convolution kernel）或者**滤波器**（filter），亦或简单地称之为该卷积层的**权重**.



### 2 卷积层

核矩阵和偏移是可学习的参数，核矩阵的大小是**超参数**

#### 2.1 二维交叉相关

![image-20250301162234639](./Convolution.assets/image-20250301162234639.png)

#### 2.2 一维和三维交叉相关

![image-20250301162419126](./Convolution.assets/image-20250301162419126.png)

- 一维可以做文本、语言和时序序列
- 三维可以做视频、医学图像和气象地图

#### 2.3 代码实现

见 [李沐学ai对应章节]([6.2. 图像卷积 — 动手学深度学习 2.0.0 documentation](https://zh-v2.d2l.ai/chapter_convolutional-neural-networks/conv-layer.html#id4))



### 3. 填充(Padding)与步幅(Stride)

