<!doctype html><html lang=zh class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Welcome to Shise's notebook. This site serves as a personal knowledge base for me to record my thoughts and ideas. It is also a place for me to share my knowledge and experience with the world. I hope you find something useful here. "><meta name=author content=ShiSeAB><link href=https://shiseab.github.io/notebook/essays/DeepSeek-R1/ rel=canonical><link href=../../nlp/Deep%20Learning%20Basic/ rel=prev><link href=../TokenSkip/ rel=next><link rel=icon href=../../assets/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.12"><title>DeepSeek-R1 - ShiSe的notebook</title><link rel=stylesheet href=../../assets/stylesheets/main.2afb09e1.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=LXGW+WenKai+Screen+GB+Screen:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"LXGW WenKai Screen GB Screen";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../css/custom.css><link rel=stylesheet href=../../css/card.css><link rel=stylesheet href=../../css/tasklist.css><link rel=stylesheet href=../../css/flink.css><link rel=stylesheet href=../../css/more_changelog.css><link rel=stylesheet href=../../css/latex.css><link rel=stylesheet href=../../css/extra.css><link rel=stylesheet href=https://gcore.jsdelivr.net/npm/lxgw-wenkai-screen-webfont@1.1.0/style.css><link rel=stylesheet href=https://gcore.jsdelivr.net/npm/lxgw-wenkai-webfont@1.1.0/style.css><link rel=stylesheet href=https://gcore.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=black data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#deepseek-r1 class=md-skip> 跳转至 </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=页眉> <a href=../.. title=ShiSe的notebook class="md-header__button md-logo" aria-label=ShiSe的notebook data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> ShiSe的notebook </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> DeepSeek-R1 </span> </div> </div> </div> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=搜索 placeholder=搜索 autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=查找> <a href=javascript:void(0) class="md-search__icon md-icon" title=分享 aria-label=分享 data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=清空当前内容 aria-label=清空当前内容 tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> 正在初始化搜索引擎 </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/ShiSeAB/notebook.git/ title=前往仓库 class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> ShiSeAB/notebook </div> </a> </div> </nav> <nav class=md-tabs aria-label=标签 data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../.. class=md-tabs__link> HOME </a> </li> <li class=md-tabs__item> <a href=../../DeepLearning/Convolution/ class=md-tabs__link> Deep Learning </a> </li> <li class=md-tabs__item> <a href=../../Compiler/Lexical%20Analysis/ class=md-tabs__link> CS课程 </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=./ class=md-tabs__link> 论文阅读 </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=导航栏 data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title=ShiSe的notebook class="md-nav__button md-logo" aria-label=ShiSe的notebook data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> ShiSe的notebook </label> <div class=md-nav__source> <a href=https://github.com/ShiSeAB/notebook.git/ title=前往仓库 class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> ShiSeAB/notebook </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_1> <div class="md-nav__link md-nav__container"> <a href=../.. class="md-nav__link "> <span class=md-ellipsis> HOME </span> </a> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=false> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> HOME </label> <ul class=md-nav__list data-md-scrollfix> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> <span class=md-ellipsis> Deep Learning </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Deep Learning </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../DeepLearning/Convolution/ class=md-nav__link> <span class=md-ellipsis> 卷积神经网络 </span> </a> </li> <li class=md-nav__item> <a href=../../DeepLearning/Modern%20CNN/ class=md-nav__link> <span class=md-ellipsis> 现代卷积神经网络架构 </span> </a> </li> <li class=md-nav__item> <a href=../../DeepLearning/Recurrent%20neural%20network/ class=md-nav__link> <span class=md-ellipsis> 循环神经网络 </span> </a> </li> <li class=md-nav__item> <a href=../../DeepLearning/Modern%20RNN/ class=md-nav__link> <span class=md-ellipsis> 现代循环神经网络 </span> </a> </li> <li class=md-nav__item> <a href="../../DeepLearning/Attention Mechanisms" class=md-nav__link> <span class=md-ellipsis> 注意力机制 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> <span class=md-ellipsis> CS课程 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> CS课程 </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_1> <label class=md-nav__link for=__nav_3_1 id=__nav_3_1_label tabindex=0> <span class=md-ellipsis> 编译原理 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_1_label aria-expanded=false> <label class=md-nav__title for=__nav_3_1> <span class="md-nav__icon md-icon"></span> 编译原理 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Compiler/Lexical%20Analysis/ class=md-nav__link> <span class=md-ellipsis> 词法分析 </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_1_2> <label class=md-nav__link for=__nav_3_1_2 id=__nav_3_1_2_label tabindex=0> <span class=md-ellipsis> 语法分析 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_3_1_2_label aria-expanded=false> <label class=md-nav__title for=__nav_3_1_2> <span class="md-nav__icon md-icon"></span> 语法分析 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Compiler/Parsing/ class=md-nav__link> <span class=md-ellipsis> Top-Down </span> </a> </li> <li class=md-nav__item> <a href=../../Compiler/Parsing%20-%202/ class=md-nav__link> <span class=md-ellipsis> Bottom-Up </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href="../../Commpiler/Abstract Syntax.md" class=md-nav__link> <span class=md-ellipsis> 抽象语法 </span> </a> </li> <li class=md-nav__item> <a href=../../Compiler/Semantic%20Analysis/ class=md-nav__link> <span class=md-ellipsis> 语义分析 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_2> <label class=md-nav__link for=__nav_3_2 id=__nav_3_2_label tabindex=0> <span class=md-ellipsis> 自然语言处理导论 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_2_label aria-expanded=false> <label class=md-nav__title for=__nav_3_2> <span class="md-nav__icon md-icon"></span> 自然语言处理导论 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../nlp/Deep%20Learning%20Basic/ class=md-nav__link> <span class=md-ellipsis> 深度学习基础 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4 checked> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex> <span class=md-ellipsis> 论文阅读 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=true> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> 论文阅读 </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> DeepSeek-R1 </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> DeepSeek-R1 </span> </a> <nav class="md-nav md-nav--secondary" aria-label=目录> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> 目录 </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#r1-zero class=md-nav__link> <span class=md-ellipsis> R1-zero: </span> </a> <nav class=md-nav aria-label=R1-zero:> <ul class=md-nav__list> <li class=md-nav__item> <a href=#rl算法 class=md-nav__link> <span class=md-ellipsis> RL算法 </span> </a> </li> <li class=md-nav__item> <a href=#reward-modeling class=md-nav__link> <span class=md-ellipsis> Reward Modeling </span> </a> </li> <li class=md-nav__item> <a href=#training-template class=md-nav__link> <span class=md-ellipsis> Training Template </span> </a> </li> <li class=md-nav__item> <a href=#performance class=md-nav__link> <span class=md-ellipsis> Performance </span> </a> </li> <li class=md-nav__item> <a href=#self-evolution-process class=md-nav__link> <span class=md-ellipsis> Self-evolution Process </span> </a> </li> <li class=md-nav__item> <a href=#aha-moment class=md-nav__link> <span class=md-ellipsis> Aha Moment </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#r1 class=md-nav__link> <span class=md-ellipsis> R1 </span> </a> <nav class=md-nav aria-label=R1> <ul class=md-nav__list> <li class=md-nav__item> <a href=#cold-start class=md-nav__link> <span class=md-ellipsis> Cold start </span> </a> </li> <li class=md-nav__item> <a href=#reasoning-oriented-rl class=md-nav__link> <span class=md-ellipsis> Reasoning-oriented RL </span> </a> </li> <li class=md-nav__item> <a href=#rejection-sampling-and-sft class=md-nav__link> <span class=md-ellipsis> Rejection Sampling and SFT </span> </a> </li> <li class=md-nav__item> <a href=#rl-for-all-scenarios class=md-nav__link> <span class=md-ellipsis> RL for all Scenarios </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#distillation class=md-nav__link> <span class=md-ellipsis> Distillation </span> </a> </li> <li class=md-nav__item> <a href=#discussiondistillation-vs-rl class=md-nav__link> <span class=md-ellipsis> Discussion：Distillation vs RL </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../TokenSkip/ class=md-nav__link> <span class=md-ellipsis> TokenSkip </span> </a> </li> <li class=md-nav__item> <a href=../DEER/ class=md-nav__link> <span class=md-ellipsis> DEER </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label=目录> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> 目录 </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#r1-zero class=md-nav__link> <span class=md-ellipsis> R1-zero: </span> </a> <nav class=md-nav aria-label=R1-zero:> <ul class=md-nav__list> <li class=md-nav__item> <a href=#rl算法 class=md-nav__link> <span class=md-ellipsis> RL算法 </span> </a> </li> <li class=md-nav__item> <a href=#reward-modeling class=md-nav__link> <span class=md-ellipsis> Reward Modeling </span> </a> </li> <li class=md-nav__item> <a href=#training-template class=md-nav__link> <span class=md-ellipsis> Training Template </span> </a> </li> <li class=md-nav__item> <a href=#performance class=md-nav__link> <span class=md-ellipsis> Performance </span> </a> </li> <li class=md-nav__item> <a href=#self-evolution-process class=md-nav__link> <span class=md-ellipsis> Self-evolution Process </span> </a> </li> <li class=md-nav__item> <a href=#aha-moment class=md-nav__link> <span class=md-ellipsis> Aha Moment </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#r1 class=md-nav__link> <span class=md-ellipsis> R1 </span> </a> <nav class=md-nav aria-label=R1> <ul class=md-nav__list> <li class=md-nav__item> <a href=#cold-start class=md-nav__link> <span class=md-ellipsis> Cold start </span> </a> </li> <li class=md-nav__item> <a href=#reasoning-oriented-rl class=md-nav__link> <span class=md-ellipsis> Reasoning-oriented RL </span> </a> </li> <li class=md-nav__item> <a href=#rejection-sampling-and-sft class=md-nav__link> <span class=md-ellipsis> Rejection Sampling and SFT </span> </a> </li> <li class=md-nav__item> <a href=#rl-for-all-scenarios class=md-nav__link> <span class=md-ellipsis> RL for all Scenarios </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#distillation class=md-nav__link> <span class=md-ellipsis> Distillation </span> </a> </li> <li class=md-nav__item> <a href=#discussiondistillation-vs-rl class=md-nav__link> <span class=md-ellipsis> Discussion：Distillation vs RL </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=deepseek-r1>DeepSeek-R1<a class=headerlink href=#deepseek-r1 title="Permanent link">&para;</a></h1> <ul> <li>R1-zero 不将SFT(论文：instruct-GPT RLHF)作为初始步骤，但推理效果良好，不过会有可读性差、语言混杂等问题，故引入R1</li> </ul> <blockquote> <p>SFT(supervised fine-tuning)监督微调，为了使LLMs生成符合人类偏好的答案，利用大量包含输入与对应正确输出的标注数据对预训练模型进行微调，缺点：依赖大量人为标注数据，且难以覆盖长链、多步推理场景</p> </blockquote> <ul> <li>R1 通过采用multi-stage training and cold-start data before RL，进一步提高了推理性能。</li> </ul> <p>一些前置知识：</p> <ul> <li>Post-training后训练：可提高推理任务准确性，且使模型符合社会主义价值观并适应用户偏好。与pre-training相比所需计算资源相对较少</li> <li> <p>OpenAI的o1通过增加CoT的长度来引入 inference-time scaling(推理时缩放？)，使得模型在推理任务中取得显著改进，但目前无人复现成功</p> </li> <li> <p>提示词工程：</p> </li> <li> <p>Zero-Shot Prompting: One Foundation Model -&gt; Prompting -&gt; Different tasks. 只问问题，不提供例子</p> </li> <li> <p>Few-Shot Prompting: By providing few examples, the LLMs can perform a new task even it is not trained on it (in-context learning). 给例子，使得大模型可以参照</p> </li> <li> <p>Chain-of-Thought Prompting: enable complex reasoning capabilities through intermediate reasoning steps. 通过在例子中体现思维链来提高大模型准确性</p> </li> <li> <p>强化学习？</p> </li> </ul> <h3 id=r1-zero>R1-zero:<a class=headerlink href=#r1-zero title="Permanent link">&para;</a></h3> <ul> <li>验证：能否仅通过强化学习（无需监督微调）激励LLMs的推理能力，self-evolution through a pure PL process</li> <li>V3-base 作为 base model，采用GRPO作为 RL framework 来提高推理性能</li> </ul> <h4 id=rl算法>RL算法<a class=headerlink href=#rl算法 title="Permanent link">&para;</a></h4> <p>通过强化学习直接对基础模型进行训练，采用 Group Relative Policy Optimization(GRPO Shao et al.,2024) 作为强化学习算法。GRPO算法的核心思想是通过采样一组输出，计算这些输出的奖励，并根据奖励的相对值来更新模型参数。以group score来估计baseline(公式看不懂……)</p> <p><img alt=image-20250330172127282 src=../DeepSeek-R1.assets/image-20250330172127282.png></p> <p>推理过程和答案分别在 <think></think> 和<answer></answer>标签内。</p> <h4 id=reward-modeling>Reward Modeling<a class=headerlink href=#reward-modeling title="Permanent link">&para;</a></h4> <p>首先，什么是奖励模型？-- 用于打分评估大模型的回答，据此更新策略。例如在RLHF中：</p> <p><img alt=image-20250330173056477 src=../DeepSeek-R1.assets/image-20250330173056477.png></p> <ul> <li>step 1: 从数据集中采样 prompt，同时人工写出希望的答案作为 ground truth，微调</li> <li>step2: 采样 prompt，送到 SFT 后的模型进行生成，得到多个答案。由人工标注，对生成的答案进行排序，训练一个 reward model。</li> <li>step3: 采样 prompt，采用 PPO 进行强化学习，模型生成答案并得到 reward model 的打分，以此更新。</li> </ul> <p>为了训练R1-zero，采用 rule-based reward system ，该系统主要由2种类型的reward构成：</p> <ul> <li>Accuracy reward: 评估回答是否正确</li> <li>Format rewards: 强制要求模型将其思考过程置于 “<think>” 和 “</think>” 标签之间</li> </ul> <p>不采用 outcome or process neural reward model ，因为 neural reward model 在large-scale RL过程中可能受 reward hacking影响，而重新训练耗费大量资源</p> <h4 id=training-template>Training Template<a class=headerlink href=#training-template title="Permanent link">&para;</a></h4> <p>设计一个简单模板（Table 1），用于引导 base model 遵循指定指令。模板要求 R1-zero 首先生成一个 <strong>推理过程</strong> ，然后给出最终答案。该模板避免了任何的 content-specific biases（mandating reflective reasoning or prompting particular problem-solving strategies），以确保我们能够在强化学习过程中准确观察到模型的自然演进过程。</p> <h4 id=performance>Performance<a class=headerlink href=#performance title="Permanent link">&para;</a></h4> <p>……</p> <p>无需任何监督微调数据的情况下获得强大的推理能力</p> <p>同时通过 majority voting 进一步提升性能</p> <h4 id=self-evolution-process>Self-evolution Process<a class=headerlink href=#self-evolution-process title="Permanent link">&para;</a></h4> <p>在训练过程中，通过 extended test-time computation ，获得了解决越来越复杂的推理任务的能力。computation中，生成数百到数千个推理tokens，使模型能够更深入地探索和完善其思维过程。</p> <p>这种自我进化最显著的特点之一，就是随着 test-time computation 的增加，复杂行为开始出现。例如 reflection（模型重新审视并重新评估之前的步骤，以及自发探索解决问题的替代方法），这些行为是模型与RL环境互动的结果，而非编程设定。</p> <h4 id=aha-moment>Aha Moment<a class=headerlink href=#aha-moment title="Permanent link">&para;</a></h4> <p>在训练 DeepSeek-R1-Zero 的过程中，一个特别有趣的现象是 “顿悟时刻” 的出现。在这个阶段，DeepSeek-R1-Zero 通过重新评估其初始方法，学会为一个问题分配更多的思考时间。</p> <p>它凸显了强化学习的强大力量和魅力：我们无需明确教导模型如何解决问题，只需为它提供正确的激励，它就能自主开发出先进的问题解决策略。</p> <p><img alt=image-20250331143211919 src=../DeepSeek-R1.assets/image-20250331143211919.png></p> <h3 id=r1>R1<a class=headerlink href=#r1 title="Permanent link">&para;</a></h3> <ul> <li>首先收集 thousands of cold-start data to fine-tune the V3-base model</li> <li>接着像R1-zero一样执行reasoning-oriented RL，在接近收敛时，通过在 RL checkpoint 进行 rejection sampling ？来创建新的SFT data，并结合 V3 在写作、事实性问答和自我认知等领域的supervised data，接着retrain V3-base</li> <li>用新数据微调以后，该 checkpoint 会再经历一次强化学习过程，同时考虑来自所有场景的prompt。</li> <li>最后得到的 checkpoint 就是 R1 ，表现与o1-1217相当</li> </ul> <h4 id=cold-start>Cold start<a class=headerlink href=#cold-start title="Permanent link">&para;</a></h4> <p>构建并收集少量 long CoT data 用于微调模型，使模型作为 intial RL actor. 收集数据策略：</p> <ul> <li>few-shot prompting with a long CoT as an example，使模型生成带 reflection 与 verification 的详细答案</li> <li>以可读格式收集 DeepSeek-R1-Zero 的输出，并通过人工标注员进行后处理来优化结果</li> </ul> <p>优势：</p> <p>提高可读性，通过在cold start数据的每个回复末尾加入一个summary，并过滤掉非 reader-friendly 的回复，来设计一个易读模式：</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a>special_token|&lt;reasoning_process&gt;|special_token|&lt;summary&gt;
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a>#reasoning_process 是针对query的CoT
</span></code></pre></div> <p>由此认为 iterative training 对推理模型而言是一种更好的方式</p> <h4 id=reasoning-oriented-rl>Reasoning-oriented RL<a class=headerlink href=#reasoning-oriented-rl title="Permanent link">&para;</a></h4> <p>微调以后，采用与R1-zero 相同的大规模强化学习训练过程，这一阶段专注于提升模型的推理能力，特别是在编码、数学、科学和逻辑推理等推理密集型任务中，这些任务涉及定义明确、有清晰解决方案的问题。</p> <p>训练过程中，观察到CoT 出现语言混杂的情况，尤其当RL prompt 涉及多种语言时。引入 language consistency reward during RL training ，通过计算思维链中目标语言词汇的比例来得到reward。ablation experiments(消融实验)表明，该调整会导致性能下降，但使得输出更符合人类偏好，使输出更具可读性。</p> <p>最后将推理任务准确性和language consistency reward相加，形成 final reward ，然后对微调模型进行RL直到其在推理任务上达到收敛。</p> <h4 id=rejection-sampling-and-sft>Rejection Sampling and SFT<a class=headerlink href=#rejection-sampling-and-sft title="Permanent link">&para;</a></h4> <p>当 reasoning-oriented RL 收敛后，我们利用结果 checkpoint 来收集用于下一轮训练的监督微调（SFT）数据。不过这部分的SFT数据主要聚焦于整合来自其他领域的数据，以增强模型在写作、角色扮演及其他通用任务方面的能力，不像初始 cold-start data 聚焦于推理。生成数据并微调模型步骤如下：</p> <ul> <li> <p>for reasoning data，构建 reasoning prompts，并对从上述 RL 训出的 checkpoint 进行 <strong>拒绝采样</strong> 以生成推理轨迹(trajectories)。在之前的阶段，仅使用了基于规则的奖励来评估数据(?)。然而，在这个阶段，通过添加其他数据来丰富数据集，其中部分数据使用 <strong>生成奖励模型</strong> (generative reward model) by 将 ground-truth 和模型预测输入 DeepSeek-V3 进行判断。此外，由于模型输出有时混乱且难以阅读，我们过滤掉了语言混杂的思维链、长篇段落和代码块。对于每个提示，我们采样多个回复，只保留正确的回复。我们总共收集了约 600k 个与推理相关的训练样本。</p> </li> <li> <p>for Non-reasoning data，如写作、事实性问答、自我认知和翻译等，我们采用 DeepSeek-V3 的流程，并复用 DeepSeek-V3 的部分监督微调（SFT）数据集。对于某些非推理任务，在通过提示回答问题之前，我们会调用 DeepSeek-V3 生成一个潜在的思维链。然而，对于像 “你好” 这样简单的查询，我们不会提供思维链作为回复。最终，我们总共收集了大约 200k 个与推理无关的训练样本。</p> </li> </ul> <p>使用上述精心整理的约 800k 个样本的数据集，对 DeepSeek-V3-Base 进行了两个 epoch 的 SFT。</p> <h4 id=rl-for-all-scenarios>RL for all Scenarios<a class=headerlink href=#rl-for-all-scenarios title="Permanent link">&para;</a></h4> <p>这是第二阶段的强化学习，目的是为了使模型进一步符合人类偏好、提高模型实用性和无害性，并优化推理能力。具体过程：结合 reward signals and diverse prompt distributions 来训模型。 对于推理数据，我们遵循 R1 - Zero 中概述的方法，利用基于规则的奖励来指导数学、编码和逻辑推理领域的学习过程。对于一般数据，我们借助奖励模型来捕捉复杂微妙场景中的人类偏好。我们以 DeepSeek - V3 的流程为基础，采用类似的偏好对和训练提示分布。</p> <p><img alt=image-20250331203704445 src=../DeepSeek-R1.assets/image-20250331203704445.png></p> <p><img alt=image-20250331203718375 src=../DeepSeek-R1.assets/image-20250331203718375.png></p> <h3 id=distillation>Distillation<a class=headerlink href=#distillation title="Permanent link">&para;</a></h3> <p>为了让更高效的小模型具备像 DeepSeek-R1 那样的推理能力，我们按照 2.3.3 节所述，使用通过 DeepSeek-R1 整理的 80 万个样本，直接对 Qwen 和 Llama 等开源模型进行微调。这种直接的蒸馏方法显著提升了小模型的推理能力。</p> <p>蒸馏过程只进行SFT，不包括RL。大模型在 RL 阶段可能出现许多高阶推理模式。而小模型因为容量和表示能力有限，很难在无监督或纯 RL 情境下学到相似水平的推理模式。</p> <p>蒸馏可将「大模型的推理轨迹」直接转移给小模型，小模型只需要模仿大模型相对完备的推理流程，可以在较小训练/推理开销下取得远胜于自身独立强化学习的效果。</p> <h3 id=discussiondistillation-vs-rl>Discussion：Distillation vs RL<a class=headerlink href=#discussiondistillation-vs-rl title="Permanent link">&para;</a></h3> <p>通过对 DeepSeek-R1 进行蒸馏，小模型能够取得令人瞩目的成果。然而，仍然存在一个问题：在不进行蒸馏的情况下，小模型能否通过本文所讨论的大规模强化学习训练达到与之相当的性能呢？</p> <p>使用数学、编码和科学、技术、工程和数学（STEM）领域的数据，对 Qwen-32B-Base 模型进行大规模的强化学习训练，训练步数超过 10k steps ，得到 DeepSeek-R1-Zero-Qwen-32B 模型，该模型明显性能比不过从 DeepSeek-R1蒸馏出来的 DeepSeek-R1-Distill-Qwen-32B 模型。</p> <p>由此，得到结论：</p> <ol> <li>将更强大的模型进行蒸馏以得到较小的模型能产生出色的效果，而较小的模型依靠本文中提到的大规模强化学习则需要巨大的计算能力，甚至可能都 <strong>无法达到</strong> 蒸馏所带来的性能水平</li> <li>虽然蒸馏策略既经济又有效，但要突破智能的界限，可能仍需要更强大的基础模型和更大规模的强化学习。</li> </ol> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> 回到页面顶部 </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=页脚> <a href=../../nlp/Deep%20Learning%20Basic/ class="md-footer__link md-footer__link--prev" aria-label="上一页: 深度学习基础"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> 上一页 </span> <div class=md-ellipsis> 深度学习基础 </div> </div> </a> <a href=../TokenSkip/ class="md-footer__link md-footer__link--next" aria-label="下一页: TokenSkip"> <div class=md-footer__title> <span class=md-footer__direction> 下一页 </span> <div class=md-ellipsis> TokenSkip </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2023 ~ now | 🚀 Chen Wu (ShiSe) </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../..", "features": ["content.code.annotate", "content.code.copy", "content.tooltips", "navigation.expand", "navigation.footer", "navigation.indexes", "navigation.path", "navigation.sections", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}, "version": null}</script> <script src=../../assets/javascripts/bundle.c8b220af.min.js></script> <script src=../../js/baidu-tongji.js></script> <script src=../../js/katex.js></script> <script src=../../js/mathjax.js></script> <script src=https://gcore.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script> </body> </html>